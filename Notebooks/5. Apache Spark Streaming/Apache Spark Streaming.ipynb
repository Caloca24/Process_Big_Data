{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico con  Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se utilizarán los datos de Kaggle de fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/fraud_detection.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'nameOrig',\n",
       " 'oldbalanceOrg',\n",
       " 'newbalanceOrig',\n",
       " 'nameDest',\n",
       " 'oldbalanceDest',\n",
       " 'newbalanceDest',\n",
       " 'isFraud',\n",
       " 'isFlaggedFraud']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #ff4fa4> <i> <b> En Notebooks anteriores vimos que .drop() quita 2 columns del dataset </b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"isFraud\", \"isFlaggedFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|   type| amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|   1|PAYMENT|9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|\n",
      "|   1|PAYMENT|1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step maps a unit of time in the real world. In this case 1 step is 1 hour of time. So we can assume for this example that we have another job that runs every hour and gets all the transactions in that time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "| 148|   12|\n",
      "|  31|   12|\n",
      "|  85|   14|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"step\").count().show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore save the output of that job by filtering on each step and saving it to a separate file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #ff4fa4> <i> <b>  %%time is a magic command available in Jupyter notebooks. It is part of the IPython kernel and is used to measure the execution time of a single cell </b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'steps = df.select(\"step\").distinct().collect()\\nfor step in steps[:]:\\n    _df = df.where(f\"step = {step[0]}\")\\n    #by adding coalesce(1) we save the dataframe to one file\\n    _df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"data/fraud\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"steps = df.select(\"step\").distinct().collect()\n",
    "for step in steps[:]:\n",
    "    _df = df.where(f\"step = {step[0]}\")\n",
    "    #by adding coalesce(1) we save the dataframe to one file\n",
    "    _df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"data/fraud\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #ff4fa4> <i> <b> ! prefix allows you to execute shell commands  straight in Jupyter Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #ff4fa4> <i> <b> :This changes the current working directory to the data/fraud folder relative to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/fraud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = spark.read.csv(\n",
    "    \"data/fraud/part-00000-897a9dd3-832b-4e43-bcdc-c0009cfec4f0-c000.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "|  34|30904|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part.groupBy(\"step\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a streaming version of this input, we'll read each file one by one as if it was a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = part.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('step', IntegerType(), True), StructField('type', StringType(), True), StructField('amount', DoubleType(), True), StructField('nameOrig', StringType(), True), StructField('oldbalanceOrg', DoubleType(), True), StructField('newbalanceOrig', DoubleType(), True), StructField('nameDest', StringType(), True), StructField('oldbalanceDest', DoubleType(), True), StructField('newbalanceDest', DoubleType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*maxFilesPerTrigger* allows you to control how quickly Spark will read all of the files in the folder. \n",
    "In this example we're limiting the flow of the stream to one file per trigger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraud', 'fraud_detection.csv', 'heart.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o80.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:265)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:36)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:198)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:212)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.csv(DataStreamReader.scala:260)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m streaming \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxFilesPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/fraud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Process_Big_Data\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:715\u001b[0m, in \u001b[0;36mDataStreamReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    683\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    684\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    712\u001b[0m     unescapedQuoteHandling\u001b[38;5;241m=\u001b[39munescapedQuoteHandling,\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    718\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    719\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    720\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Process_Big_Data\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Process_Big_Data\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Process_Big_Data\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o80.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:265)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:36)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:198)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:212)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.csv(DataStreamReader.scala:260)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"data/fraud/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a transformation.\n",
    "\n",
    "The nameDest column is the recipient ID of the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_count = streaming.groupBy(\"nameDest\").count().orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our transformation, we need to specify an output sink for the results. For this example, we're going to write to a memory sink which keeps the results in memory.\n",
    "\n",
    "We also need to define how Spark will output that data. In this example, we'll use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n",
    "\n",
    "In this example we won't include activityQuery.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active.\n",
    "\n",
    "So in order to be able to run this locally in a notebook we won't include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C319921943|    2|\n",
      "| C803352127|    2|\n",
      "|C1887077333|    2|\n",
      "| C763794011|    2|\n",
      "| C488343370|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C587204551|    2|\n",
      "|C1015743493|    2|\n",
      "| C359227905|    2|\n",
      "|C1850343194|    2|\n",
      "| C379236140|    2|\n",
      "| C319921943|    2|\n",
      "|C1377194794|    2|\n",
      "|C1455885936|    2|\n",
      "| C803352127|    2|\n",
      "| C325257804|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C587204551|    2|\n",
      "|C1015743493|    2|\n",
      "| C359227905|    2|\n",
      "|C1850343194|    2|\n",
      "| C379236140|    2|\n",
      "| C319921943|    2|\n",
      "|C1377194794|    2|\n",
      "|C1455885936|    2|\n",
      "| C803352127|    2|\n",
      "| C325257804|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C587204551|    2|\n",
      "|C1015743493|    2|\n",
      "| C359227905|    2|\n",
      "|C1850343194|    2|\n",
      "| C379236140|    2|\n",
      "| C319921943|    2|\n",
      "|C1377194794|    2|\n",
      "|C1455885936|    2|\n",
      "| C803352127|    2|\n",
      "| C325257804|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activityQuery = (\n",
    "    dest_count.writeStream.queryName(\"dest_counts\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# include this in production\n",
    "# activityQuery.awaitTermination()\n",
    "\n",
    "import time\n",
    "\n",
    "for x in range(50):\n",
    "    _df = spark.sql(\n",
    "        \"SELECT * FROM dest_counts WHERE nameDest != 'nameDest' AND count >= 2\"\n",
    "    )\n",
    "    if _df.count() > 0:\n",
    "        _df.show(10)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if stream is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active[0].isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we  want to turn off the stream we'll run activityQuery.stop() to reset the query for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
